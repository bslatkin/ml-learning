Start training
Epoch=1, Examples=100, AvgError=0.2250333967
Wrote profile results to mnist_model.py.lprof
Timer unit: 1e-06 s

Total time: 5.94428 s
File: /Users/bslatkin/projects/ml-learning/neural_net.py
Function: backward at line 73

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    73                                               @profile
    74                                               def backward(self, last_input_matrix, output_error_matrix, config):
    75                                                   # print(f'{self.__class__.__name__}{id(self)}: OutputError={output_error}')
    76
    77         3          2.0      0.7      0.0          batch_size = len(output_error_matrix)
    78
    79                                                   # Output error is ∂E/∂Y, each row is a batch, and each column in each
    80                                                   # row is the error gradient for that output position. But the biases
    81                                                   # in this FullyConnected layer are merely a vector. So this sums all
    82                                                   # of the gradients for each output positions across all batches
    83                                                   # in order to calculate the error gradient for the bias update.
    84                                                   # This took me a long time to figure out! This post helped:
    85                                                   # https://stats.stackexchange.com/questions/373163/how-are-biases-updated-when-batch-size-1
    86         3          0.0      0.0      0.0          bias_error = []
    87       163         18.0      0.1      0.0          for j in range(self.output_count):
    88       160         12.0      0.1      0.0              bias_error_j = 0
    89     16160       1270.0      0.1      0.0              for i in range(batch_size):
    90     16000       1457.0      0.1      0.0                  output_error_ij = output_error_matrix[i][j]
    91     16000       1623.0      0.1      0.0                  bias_error_j += output_error_ij
    92       160         23.0      0.1      0.0              bias_error.append(bias_error_j)
    93
    94                                                   # Same size as the weights matrix
    95                                                   # Rows are number of inputs
    96                                                   # Columns are number of outputs
    97                                                   # X^T * ∂E/∂Y
    98         3          0.0      0.0      0.0          weights_error = []
    99       937         82.0      0.1      0.0          for i in range(self.input_count):
   100       934         70.0      0.1      0.0              weights_i = []
   101     84834       6355.0      0.1      0.1              for j in range(self.output_count):
   102     83900       9976.0      0.1      0.2                  weights_i.append(0)
   103       934        160.0      0.2      0.0              weights_error.append(weights_i)
   104
   105                                                   # x has one row per batch with columns being input values
   106                                                   # x^t has each batch as a column, so each row is the i-th input across all batches
   107
   108                                                   # ∂E/∂Y has one row per batch, each column is the j-th output
   109                                                   # the weight error at each point ij is the dot product
   110                                                   # of the i-th input across all batches with the j-th output
   111                                                   # across all batches.
   112       937         93.0      0.1      0.0          for i in range(self.input_count):
   113     84834       7098.0      0.1      0.1              for j in range(self.output_count):
   114     83900       5369.0      0.1      0.1                  delta = 0
   115
   116   8473900     606359.0      0.1     10.2                  for batch_index in range(batch_size):
   117   8390000     812992.0      0.1     13.7                      input_bi = last_input_matrix[batch_index][i]
   118   8390000     768570.0      0.1     12.9                      output_error_bj = output_error_matrix[batch_index][j]
   119   8390000     894137.0      0.1     15.0                      delta += input_bi * output_error_bj
   120
   121     83900      11502.0      0.1      0.2                  weights_error[i][j] = delta
   122
   123                                                   # print(f'{self.__class__.__name__}{id(self)}: WeightsError={weights_error}')
   124
   125                                                   # Same size as the input
   126                                                   # Rows are batches
   127                                                   # Columns are inputs
   128                                                   # ∂E/∂Y * W^T
   129         3          0.0      0.0      0.0          input_error = []
   130       303         22.0      0.1      0.0          for batch_index in range(batch_size):
   131       300         31.0      0.1      0.0              input_error_b = []
   132     93700       7228.0      0.1      0.1              for i in range(self.input_count):
   133     93400      10886.0      0.1      0.2                  input_error_b.append(0)
   134       300         48.0      0.2      0.0              input_error.append(input_error_b)
   135
   136       937         98.0      0.1      0.0          for i in range(self.input_count):
   137     84834       7774.0      0.1      0.1              for j in range(self.output_count):
   138     83900      13780.0      0.2      0.2                  weight_ij = self.weights[i][j]
   139
   140   8473900     621498.0      0.1     10.5                  for batch_index in range(batch_size):
   141   8390000     781094.0      0.1     13.1                      output_error_bj = output_error_matrix[batch_index][j]
   142   8390000    1339343.0      0.2     22.5                      input_error[batch_index][i] += output_error_bj * weight_ij
   143
   144                                                   # print(f'{self.__class__.__name__}{id(self)}: InputError={input_error}')
   145
   146                                                   # Update the biases
   147       163         11.0      0.1      0.0          for j in range(self.output_count):
   148       160         10.0      0.1      0.0              bias_error_j = bias_error[j]
   149       160         34.0      0.2      0.0              self.biases[j] -= config.learning_rate * bias_error_j
   150
   151                                                   # Update weights
   152       937         76.0      0.1      0.0          for i in range(self.input_count):
   153     84834       6245.0      0.1      0.1              for j in range(self.output_count):
   154     83900       8197.0      0.1      0.1                  weights_error_ij = weights_error[i][j]
   155     83900       7637.0      0.1      0.1                  delta = config.learning_rate * weights_error_ij
   156     83900      13099.0      0.2      0.2                  self.weights[i][j] -= delta
   157
   158         3          0.0      0.0      0.0          return input_error

